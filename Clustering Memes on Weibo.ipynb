{
 "metadata": {
  "name": "Clustering Memes on Weibo"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Clustering Memes on Sina Weibo"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will describe the prototype for memes analysis in a large tweets corpus that includes detection of memes (clustering), localization in Chinese (NLP), geo-entities (NER, geotag) and visualization for classification."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Get Data : tweet corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To create this project we will use the data provided by the project **Weiboscope** from *HKU University, JMSC* - [link](http://147.8.142.179/datazip/). \n",
      "\n",
      "The dataset contains sample data from 52 weeks of 2012 from more than 350,000 Chinese microbloggers who have more than 1,000 followers (Fu, Chan, Chau, 2013 ; Fu, Chau, 2013).\n",
      "\n",
      "Note : this data has been anonymized"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Download the dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "$ bash bin/get_data.sh\n",
      "# Done in 58 min for 57 files, 18G"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Set Statistics:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\t* Number of weibo messages: 226841122\n",
      "\t* Number of deleted messages: 10865955\n",
      "\t* Number of censored ('Permission Denied') messages: 86083\n",
      "\t* Number of unique weibo users: 14387628"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prepare Data : extract protomemes from tweets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To detect memes in this large corpus, we design a clustering algorithm using the concept of protomeme ([Ferrara, 2013](http://www.emilio.ferrara.name/2013/08/01/clustering-memes-in-social-media/)). \n",
      "\n",
      "Protomemes are minimum units contained in tweets :\n",
      "\n",
      "* hashtags\n",
      "* urls\n",
      "* mentions/RT\n",
      "* text\n",
      "\n",
      "To extract all protomemes, we use map-reduce via MongoDB implementation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "$ bash bin/prepare.sh data/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ref : *Ferrara, E., JafariAsbagh, M., Varol, O., Qazvinian, V., Menczer, F., & Flammini, A. (2013). Clustering Memes in Social Media. Computers and Society; Data Analysis, Statistics and Probability; Physics and Society. doi:10.1145/2492517.2492530*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Compare Protomemes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To identify memes, we look in the whole dataset for interesting clusters of protomemes. In order to achieve this we compare different element of protomemes unsing similarity measures, then we add for each protomeme those measures into a single one and use a hierachical linkage algoritm to identify clusters."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Compute similarities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each protomemes, we compute the (cosine) similiarity on different vector spaces as follow :\n",
      "\n",
      "* txt_similarity : tweet text using TF-IDF \n",
      "* tweets_similarity : hashtags/urls only\n",
      "* user_similarity : users (binary) vectors \n",
      "* diffusion_similarity : graph of the conversation as (binary) vector"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Combine similarities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Those different similarities values are combined into a single using weighted index :\n",
      "\n",
      "    wt = 0.0 , wc = 0.7 , wu = 0.1 , wd = 0.2\n",
      "\tcombined_index = wc*txt_sim + wd*diff_sim + wt*tweets_sim + wu*users_sim"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Clustering & Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the combined index, we use k-means and MAX algorithm to identify clusters within the protomemes set and define memes.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Memes analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have identify the clusters into our datasets, we extract some relevant datasets for the most interesting memes and we will explore them."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Meme datasets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After the clustering process, we extract all messages contains in each protomemes to create specific datasets for each memes. For each messages in those memes, we collect some information (geo-entities, geo-tags, conversation graph, etc.) and we proceed towards visualization of the memes."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Semantic Parsing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each memes, we collect all tweets and we extract named entities using Stanford NER\n",
      "\n",
      "* Segmentation using Guokr's [segmentation tool](https://github.com/guokr/gkseg) and training [corpus](https://github.com/guokr/corpus)\n",
      "* NER using Stanford Parser\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Visualization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To visualize our memes, this seems to look good : http://bokeh.pydata.org/"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}