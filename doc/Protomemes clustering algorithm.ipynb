{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Collect relevant tweets using protomemes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To detect memes in a large corpus, some recent researches propose to implement the concept of protomemes as a clustering algorithm [(Ferrara, 2013)](http://www.emilio.ferrara.name/2013/08/01/clustering-memes-in-social-media/). **Protomeme** is an abbreviation for \"prototype meme\", a meme under construction [(Gabora, 1997)](http://cfpm.org/jom-emit/1997/vol1/gabora_l.html). In the context of social media, we can describe protomemes as minimum units contained in tweets :\n",
      "\n",
      "* hashtags\n",
      "* urls\n",
      "* mentions/RT\n",
      "* text\n",
      "\n",
      "By looking at the evolution of protomemes, we can identify memes in formation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Store tweets in mongoDB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import bson.errors.InvalidStringData:\n",
      "from time import time\n",
      "from models.tweet import Tweet # mongo data model\n",
      "import lib.tweetminer as minetweet\n",
      "\n",
      "# Connect to Mongo\n",
      "db=MongoDB(\"weibodata\").db\n",
      "\n",
      "# Download files (56GB, make take some time)\n",
      "# os.system('wget -r \"http://147.8.142.179/datazip/\"')\n",
      "\n",
      "# Where are the downloaded raw files\n",
      "raw_path=os.path.dirname(os.path.abspath(__file__)) +\"/data/datazip/\"\n",
      "\n",
      "# Init libraries\n",
      "nlp=NLPMiner()\n",
      "\n",
      "# scan all downloaded files\n",
      "csvfiles = [ os.path.join(raw_path,f) for f in os.listdir(raw_path) if os.path.isfile(os.path.join(raw_path,f)) ]\n",
      "\n",
      "for csv_file in csv_files:\n",
      "    \n",
      "    # extract_and_store_tweets(csv_file,nlp, minetweet)\n",
      "    t0=time() # measure time\n",
      "    \n",
      "    with open(csvfile, 'r') as f:\n",
      "        next(f) # skip csv header\n",
      "        data = csv.reader(f)\n",
      "\n",
      "        # one row at a time\n",
      "        for row in data: \n",
      "\n",
      "            # create Tweet object\n",
      "            t=Tweet()\n",
      "\n",
      "            # Populate Tweet\n",
      "            t.mid=row[0]\n",
      "            t.retweetFromPostId=row[1]\n",
      "            t.userId=row[2]\n",
      "            t.retweetFromUserId=row[3]\n",
      "            t.source=row[4]\n",
      "            t.hasImage=row[5]\n",
      "            t.txt=row[6]\n",
      "            t.geo=row[7]\n",
      "            t.created_at=row[8]\n",
      "            t.deleted_last_seen=row[9]\n",
      "            t.permission_denied=row[10]\n",
      "                        \n",
      "            # Extract tweet entities\n",
      "            t.mentions,t.urls,t.hashtags,clean=minetweet.extract_tweet_entities(t.txt)\n",
      "\n",
      "            # Extract keywords\n",
      "            dico=nlp.extract_dictionary(clean)\n",
      "            \n",
      "            # Remove stopwords and store clean dico\n",
      "            t.dico=nlp.remove_stopwords(dico)\n",
      "            \n",
      "            # Extract entities (NER server should be started - see lib/ner-server)\n",
      "            # t.entities=nlp.extract_named_entities_from_dico(t.dico)\n",
      "            \n",
      "            # Check encoding problems\n",
      "            valid_utf8 = True\n",
      "            try:\n",
      "                t.txt.decode('utf-8')\n",
      "            except UnicodeDecodeError:\n",
      "                unvalid_tweets+=1\n",
      "                valid_utf8 = False\n",
      "                print ' bad encoding : tweet ',t.mid\n",
      "                # pprint(t)\n",
      "                \n",
      "            # Save tweet\n",
      "            if valid_utf8 is True:\n",
      "                try:\n",
      "                    t.save() # save to mongo\n",
      "                    tweets_count+=1\n",
      "                except InvalidStringData:\n",
      "                    print ' bad encoding : tweet ',t.mid    \n",
      "    \n",
      "    print \" done in %fs\" % (time() - t0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Create protomemes set from the tweets corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once all our tweets are stored in the database, we use Mongo map-reduce algorithm (not fast) to collect tweets and create protomeme-specific datasets (for each hashtag, url and mentions).\n",
      "\n",
      "\n",
      "See ```extract_protomemes_using_multiple_processes``` function in  ```lib/protomemes.py``` for an optimized version using multi-processing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from time import time\n",
      "from bson.code import Code\n",
      "from lib.mongo import MongoDB\n",
      "\n",
      "# Define the Mongo collection where the raw data is stored\n",
      "source_collection=\"tweets\"\n",
      "\n",
      "# Connect to Mongo\n",
      "db=MongoDB(\"weibodata\").db\n",
      "\n",
      "# Start\n",
      "t0 =time()\n",
      "\n",
      "# Get corpus length\n",
      "tweets_count=db[source_collection].count()\n",
      "print str(tweets_count)+\" tweets in the db\"\n",
      "\n",
      "# Define collections to be created\n",
      "pm_collections= [\"hashtags\", \"mentions\", \"urls\"]\n",
      "\n",
      "# import JS code to use Mongo native map-reduce\n",
      "mapjs=open(os.path.dirname(os.path.abspath(__file__)) +\"lib/mapreduce/map.js\", \"r\").read()\n",
      "reducejs=open(os.path.dirname(os.path.abspath(__file__)) +\"lib/mapreduce/reduce.js\", \"r\").read()\n",
      "\n",
      "for collection_name in pm_collections : \n",
      "\n",
      "    # compile code to BSON\n",
      "    mapper = Code(mapjs.replace(\"TO_BE_CHANGED\", collection_name)) # change collection type within the js code\n",
      "    reducer = Code(reducejs)\n",
      "\n",
      "    # Process Mongo map-reduce\n",
      "    result = db[source_collection].map_reduce(mapper, reducer, collection_name, limit=tweets_count)\n",
      "    \n",
      "    print \" %d new protomemes extracted\" % result.count()\n",
      "    print \" stored in collection : %s \" % db[_destination]\n",
      "    print \" done in %fs\" % (time() - t0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Compute similarities between protomemes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have specific dataset for each protomemes, we want to identify memes within them. To detect clusters, we will compute different similiarities between each protomemes set to compare them and find the most similar. Four similarities values are defined as follow :\n",
      "\n",
      "* **txt_similarity** : cosine similarities between tweet text vectors using TF-IDF \n",
      "* **diffusion_similarity** : cosine similarities between conversation graphs as vectors\n",
      "* **tweets_similarity** : cosine similarities between hashtags/urls as vectors \n",
      "* **user_similarity** : cosine similarities between users (binary) vectors \n",
      "\n",
      "We use a linear combination to combine those similarities into a single index. Scalars are use to weight each sim value, following Ferrara's original paper : \n",
      "\n",
      "\twt = 0.0 , wc = 0.7 , wu = 0.1 , wd = 0.2\n",
      "\tcombined_index = wc*txt_sim + wd*diff_sim + wt*tweets_sim + wu*users_sim"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Extract raw corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also store corpora as files to keep the process memory-friendly"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lib.mongo import MongoDB\n",
      "\n",
      "# Reference different types of corpus to be extracted\n",
      "types=[\"txt\",\"diffusion\",\"tweets\",\"users\"]\n",
      "\n",
      "# Temporary directory to store all indexes\n",
      "tmp_path=\"/tmp\"\n",
      "\n",
      "# Collections to be used\n",
      "pm_collections= [\"hashtags\", \"mentions\", \"urls\"]\n",
      "\n",
      "# Connect to Mongo\n",
      "db=MongoDB(\"weibodata\").db\n",
      "\n",
      "# Start\n",
      "t0 =time()\n",
      "\n",
      "for collection in pm_collections: \n",
      "    \n",
      "    # Get corpus length\n",
      "    tweets_count=db[source_collection].count()\n",
      "    print str(tweets_count)+\" tweets in the db\"\n",
      "\n",
      "    # Create raw corpus for each protomeme \n",
      "    for t in types:\n",
      "        filename=_path+\"/protomemes.\"+type\n",
      "    \n",
      "        # apply treshold with at least 5 tweets and 5 users\n",
      "        query1={\n",
      "            \"value.tweets.5\": { \"$exists\":\"true\"},\n",
      "            \"value.users.5\": { \"$exists\":\"true\"}\n",
      "        }\n",
      "        \n",
      "        # get only specific type\n",
      "        query2 = { \"value.\"+_type : 1 }\n",
      "        \n",
      "        data=db[source_collection].find(query1, query2).limit(tweets_count)\n",
      "        print ' got %d records'%len(data)\n",
      "        \n",
      "        # open file\n",
      "        with codecs.open(filename, \"w\", \"utf-8\") as outfile:\n",
      "            for item in data:\n",
      "            outfile.write(str(item[\"value\"][\"txt\"].split())[1:-1]+\"\\n\")\n",
      "        outfile.close()\n",
      "        \n",
      "        print ' %s done '%t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###\u00a0Extract corpus and compute similarities\n",
      "\n",
      "To compute the similarities between different protomemes, we follow those steps :"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Compute similarity for each corpus"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Create the combined similarities index"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4. Extract the most similar into memes"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}