{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Time-based data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "import os, csv, json\n",
      "from time import time \n",
      "import datetime\n",
      "from collections import Counter\n",
      "import lib.tweetminer as minetweet\n",
      "from lib.users import UserAPI\n",
      "import networkx as nx\n",
      "import community\n",
      "from lib.nlp import NLPMiner\n",
      "import locale\n",
      "\n",
      "results_path=\"/home/clemsos/Dev/mitras/results/\"\n",
      "meme_names=[\"biaoge\"]\n",
      "# meme_names=[ meme for meme in os.listdir(results_path) if meme[-3:] != \"csv\"]\n",
      "# meme_names=[\n",
      "#  'biaoge',\n",
      "#  'thevoice',\n",
      "#  'moyan',\n",
      "#  'hougong',\n",
      "#  'gangnam',\n",
      "#  'sextape',\n",
      "#  'dufu',\n",
      "#  'ccp',\n",
      "#  'yuanfang',\n",
      "#  'qiegao']\n",
      "\n",
      "\n",
      "print meme_names\n",
      "\n",
      "\n",
      "t0=time()\n",
      "minetweet.init_tweet_regex()\n",
      "\n",
      "\n",
      "locale.setlocale(locale.LC_ALL, \"\")\n",
      "\n",
      "nlp=NLPMiner()\n",
      "\n",
      "stoplist=[i.strip() for i in open(\"lib/stopwords/zh-stopwords\",\"r\")]\n",
      "stoplist+=[i.strip() for i in open(\"lib/stopwords/stopwords.txt\",\"r\")]\n",
      "stoplist+=[\"\u8f6c\u53d1\",\"\u5fae\u535a\",\"\u8bf4 \",\"\u4e00\u4e2a\",\"\u3010 \",\"\u5e74 \",\"\u8f6c \",\"\u8bf7\",\"\uff02 \",\"\u95ee\u9898\",\"\u77e5\u9053\",\"\u4e2d \",\"\u5df2\u7ecf\",\"\u73b0\u5728\",\"\u8bf4\",\"\u3010\",'\uff02',\"\u5e74\",\"\u4e2d\",\"\u4eca\u5929\",\"\u5e94\u8be5\",\"\u771f\u7684\",\"\u6708\",\"\u5e0c\u671b\",\"\u60f3\",\"\u65e5\",\"\u8fd9\u662f\",\"\u592a\",\"\u8f6c\",\"\u652f\u6301\"]\n",
      "# stoplist+=[\"\u4e8b\u513f\",\"\u4e2d\u56fd\"]\n",
      "\n",
      "\n",
      "api=UserAPI()\n",
      "words_users_time=[]\n",
      "\n",
      "def get_province(_userid):\n",
      "    province_code= api.get_province(_userid)\n",
      "    # print province_code\n",
      "    try :\n",
      "        return api.provinces[province_code]\n",
      "    except KeyError :\n",
      "        return 0\n",
      "        pass\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "init tweet entities regex\n",
        "['biaoge']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "init tweet entities regex\n",
        "init NLP toolkit\n",
        "User API instance\n",
        "\n",
        "Connecting to MongoDB... \n",
        "Connected successfully MongoDB at localhost:27017\n",
        "\n",
        "Total users in the db : 14388484\n",
        "------------\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "meme_name=\"biaoge\"\n",
      "\n",
      "# Init\n",
      "# tstart=time()\n",
      "print \"Processing meme '%s'\"%meme_name\n",
      "\n",
      "# files names\n",
      "meme_path=outfile=results_path+meme_name\n",
      "meme_csv=meme_path+\"/\"+meme_name+\".csv\"\n",
      "\n",
      "jsondata={}\n",
      "jsondata[\"meme_name\"]=meme_name\n",
      "\n",
      "users=[]\n",
      "users_edges=[]\n",
      "\n",
      "words=[]\n",
      "words_users=[]\n",
      "words_edges={}\n",
      "\n",
      "count=0\n",
      "\n",
      "#\u00a0 words_time={}\n",
      "# user_edges_time=[]\n",
      "# words_users_time={}\n",
      "\n",
      "by_time={}\n",
      "\n",
      "# process the data\n",
      "with open(meme_csv, 'rb') as csvfile:\n",
      "    memecsv=csv.reader(csvfile)\n",
      "    memecsv.next() # skip headers\n",
      "\n",
      "    for row in memecsv:\n",
      "        # extract text\n",
      "        t=row[1]    \n",
      "        count+=1\n",
      "        \n",
      "        # time (round and store)\n",
      "        d=datetime.datetime.strptime(row[9], \"%Y-%m-%dT%H:%M:%S\")\n",
      "        day = datetime.datetime(d.year,d.month,d.day,d.hour,0,0)\n",
      "        timestamp=day.strftime(\"%s\")\n",
      "        \n",
      "        # regexp extract tweet entities\n",
      "        mentions,urls,hashtags,clean=minetweet.extract_tweet_entities(t)\n",
      "                  \n",
      "        # User diffusion graph\n",
      "        user_diff=[]\n",
      "        users_to_users=[]\n",
      "        for mention in mentions:\n",
      "            users_to_users.append((row[0],mention))\n",
      "            # user_edges_time.append((row[0],mention,timestamp))\n",
      "            if mention not in user_diff : user_diff.append(mention)\n",
      "\n",
      "            # retweeted_uid\n",
      "        if row[7] != \"\" : \n",
      "            users_to_users.append((row[7],row[0]))\n",
      "            # user_edges_time.append((row[7],row[0],timestamp))\n",
      "            if row[7] not in user_diff : user_diff.append(row[7])\n",
      "        \n",
      "        users_edges+=users_to_users # store all interactions\n",
      "        users+=user_diff # store all users\n",
      "        \n",
      "        # extract text \n",
      "        dico=nlp.extract_dictionary(clean)\n",
      "\n",
      "        # remove stopwords and get clean dico\n",
      "        clean_dico=nlp.remove_stopwords(dico)\n",
      "        \n",
      "        # remove more stopwords\n",
      "        tmp_words=[w for w in clean_dico if w.encode('utf-8') not in stoplist and w[0] != \"u\" ]\n",
      "        words+=tmp_words # global list for counter  \n",
      "        \n",
      "        # words edges\n",
      "        words_to_words=[]\n",
      "        words_to_users=[]\n",
      "        for w in tmp_words :\n",
      "            \n",
      "            # word edges\n",
      "            words_to_words+=[(w,t) for t in tmp_words if t!=w]\n",
      "            \n",
      "            # word to users\n",
      "            words_to_users+=[(w,u) for u in user_diff]\n",
      "            \n",
      "            try: words_edges[w]\n",
      "            except KeyError: words_edges[w]=[]\n",
      "            words_edges[w]+=[(w,t) for t in tmp_words if t!=w]\n",
      "        \n",
      "        # words_edges+=words_to_words\n",
      "        words_users+=words_to_users\n",
      "        \n",
      "        # store data by time\n",
      "        try : by_time[timestamp]\n",
      "        except KeyError: by_time[timestamp]={}\n",
      "            \n",
      "        # count\n",
      "        try : by_time[timestamp][\"count\"]\n",
      "        except KeyError: by_time[timestamp][\"count\"]=0\n",
      "        by_time[timestamp][\"count\"]+=1\n",
      "        \n",
      "        # users edges\n",
      "        try: by_time[timestamp][\"user_edges\"]\n",
      "        except KeyError: by_time[timestamp][\"user_edges\"]=[]\n",
      "        by_time[timestamp][\"user_edges\"]+=users_to_users\n",
      "        \n",
      "        # users nodes\n",
      "        try: by_time[timestamp][\"user_nodes\"]\n",
      "        except KeyError: by_time[timestamp][\"user_nodes\"]=[]\n",
      "        by_time[timestamp][\"user_nodes\"]+=user_diff\n",
      "        \n",
      "        # words nodes\n",
      "        try: by_time[timestamp][\"words_nodes\"]\n",
      "        except KeyError: by_time[timestamp][\"words_nodes\"]=[]\n",
      "        by_time[timestamp][\"words_nodes\"]+=tmp_words\n",
      "        \n",
      "        # word edges\n",
      "        try: by_time[timestamp][\"words_edges\"]\n",
      "        except KeyError: by_time[timestamp][\"words_edges\"]=[]\n",
      "        by_time[timestamp][\"words_edges\"]+=words_to_words\n",
      "        \n",
      "        # word edges\n",
      "        try: by_time[timestamp][\"words_to_users\"]\n",
      "        except KeyError: by_time[timestamp][\"words_to_users\"]=[]\n",
      "        by_time[timestamp][\"words_to_users\"]+=words_to_users\n",
      "\n",
      "print \"processing done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Building Trie..., from /home/clemsos/Dev/mitras/lib/dict/dict.txt.big\n",
        "dumping model to file cache /tmp/jieba.user.3946248680419969172.cache"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "loading model cost "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing meme 'biaoge'\n",
        "processing done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 5.82669115067 seconds.\n",
        "Trie has been built succesfully.\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parse provinces for all users\n",
      "\n",
      "user_provinces={}\n",
      "unique_users=[u[0] for u in Counter(users).most_common()]\n",
      "for user in unique_users:\n",
      "    province=get_province(user)\n",
      "    user_provinces[user]=province"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# User graph info\n",
      "print \"USER GRAPH\"\n",
      "print \"-\"*20\n",
      "print \"Edges (total number) : %d edges\"%len(users_edges)\n",
      "\n",
      "# remove users edges that have a minium value of minimum_exchange\n",
      "graphsize=[c[1] for c in Counter(users_edges).most_common()]\n",
      "occurences=Counter(graphsize).most_common()\n",
      "# print occurences\n",
      "minimum_exchange=1\n",
      "\n",
      "# create graph object\n",
      "edges_weighted=[str(p[0][0]+\" \"+p[0][1]+\" \"+str(p[1])) for p in Counter(users_edges).most_common() if p[1] > minimum_exchange]\n",
      "print \"Weighted edges %d\"%len(edges_weighted)\n",
      "\n",
      "G = nx.read_weighted_edgelist(edges_weighted, nodetype=str, delimiter=\" \",create_using=nx.DiGraph())\n",
      "\n",
      "# dimensions\n",
      "N,K = G.order(), G.size()\n",
      "print \"Nodes: \", N\n",
      "print \"Edges: \", K\n",
      "\n",
      "allowed_users=G.nodes()\n",
      "\n",
      "# Average degree\n",
      "avg_deg = float(K)/N\n",
      "print \"Average degree: \", avg_deg\n",
      "\n",
      "# Average clustering coefficient\n",
      "ccs = nx.clustering(G.to_undirected())\n",
      "avg_clust_coef = sum(ccs.values()) / len(ccs) \n",
      "print \"Average clustering coeficient: %f\"%avg_clust_coef\n",
      "    \n",
      "# Communities\n",
      "user_communities = community.best_partition(G.to_undirected()) \n",
      "modularity=community.modularity(user_communities,G.to_undirected())\n",
      "print \"Modularity of the best partition: %f\"%modularity\n",
      "print \"Number of partitions : \", len(set(user_communities.values()))\n",
      "\n",
      "# betweeness_centrality\n",
      "print \"computing betweeness_centrality... (this may take some time)\"\n",
      "users_btw_cent=nx.betweenness_centrality (G.to_undirected())\n",
      "print \"computing done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "USER GRAPH\n",
        "--------------------\n",
        "Edges (total number) : 4471 edges\n",
        "Weighted edges 323"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nodes:  353\n",
        "Edges:  323\n",
        "Average degree:  0.915014164306\n",
        "Average clustering coeficient: 0.061393\n",
        "Modularity of the best partition: 0.963312"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of partitions :  118\n",
        "computing betweeness_centrality... (this may take some time)\n",
        "computing done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# WORD graph info\n",
      "print \"WORD GRAPH\"\n",
      "print \"-\"*20\n",
      "print \"%d words edges\"%len(words_edges)\n",
      "\n",
      "words_edges_weighted=[]\n",
      "words_minimum_exchange=200\n",
      "top_words_limit=500\n",
      "\n",
      "words_allowed=[c[0] for c in Counter(words).most_common(top_words_limit)]\n",
      "print \"%d words_allowed\"%len(words_allowed)\n",
      "\n",
      "for word in words_edges:\n",
      "    if word in words_allowed:\n",
      "        targets=[(c[0][1],c[1]) \n",
      "                 for c in Counter(words_edges[word]).most_common() \n",
      "                 if  c[0][0] in words_allowed\n",
      "                 and c[0][1] in words_allowed\n",
      "                 and  c[1]>words_minimum_exchange\n",
      "                 ]\n",
      "        words_edges_weighted+=[(word,w[0],w[1]) for w in targets]\n",
      "    \n",
      "print \"Words weighted edges %d\"%len(words_edges_weighted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WORD GRAPH\n",
        "--------------------\n",
        "12853 words edges\n",
        "500 words_allowed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Words weighted edges 846"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "medians=[]\n",
      "for word in words_edges:\n",
      "    if word in words_allowed:\n",
      "        nn=[c[1] for c in Counter(words_edges[word]).most_common()  \n",
      "             if  c[0][0] in words_allowed\n",
      "             and c[0][1] in words_allowed]\n",
      "        \n",
      "        medians.append(np.median(np.array(nn)))\n",
      "\n",
      "m=np.median(np.array(medians))\n",
      "print m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "6.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6.0\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordIndex={}\n",
      "for i,w in enumerate(words_allowed): wordIndex[w]=i;\n",
      "\n",
      "words_edges_weightedlist=[str(wordIndex[w[0]])+\" \"+str(wordIndex[w[1]])+\" \"+str(w[2]) for w in words_edges_weighted]    \n",
      "# print words_edges_weightedlist\n",
      "\n",
      "Gw = nx.read_weighted_edgelist(words_edges_weightedlist, nodetype=str, delimiter=\" \",create_using=nx.DiGraph())\n",
      "\n",
      "# dimensions\n",
      "Nw,Kw = Gw.order(), Gw.size()\n",
      "print \"Nodes: \", Nw\n",
      "print \"Edges: \", Kw\n",
      "\n",
      "# Average degree\n",
      "words_avg_deg = float(Kw)/Nw\n",
      "print \"Average degree: \", words_avg_deg\n",
      "\n",
      "# Average clustering coefficient\n",
      "ccsw = nx.clustering(Gw.to_undirected())\n",
      "words_avg_clust_coef = sum(ccsw.values()) / len(ccsw) \n",
      "print \"Average clustering coeficient: %f\"%words_avg_clust_coef\n",
      "    \n",
      "# Communities\n",
      "words_communities = community.best_partition(Gw.to_undirected()) \n",
      "words_modularity=community.modularity(words_communities,Gw.to_undirected())\n",
      "print \"Modularity of the best partition: %f\"%words_modularity\n",
      "print \"Number of partitions : \", len(set(words_communities.values()))\n",
      "\n",
      "# betweeness_centrality\n",
      "print \"computing betweeness_centrality... (this may take some time)\"\n",
      "words_btw_cent=nx.betweenness_centrality (Gw.to_undirected())\n",
      "print \"computing done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Nodes:  312\n",
        "Edges:  2660\n",
        "Average degree:  8.52564102564\n",
        "Average clustering coeficient: 0.659897"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Modularity of the best partition: 0.590388"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of partitions :  8\n",
        "computing betweeness_centrality... (this may take some time)\n",
        "computing done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "int('haha')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "invalid literal for int() with base 10: 'haha'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-47-3b10b7aeec32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haha'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'haha'"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parse data using time reference\n",
      "\n",
      "timeframes=[]\n",
      "word_median=5\n",
      "word_edge_median=10\n",
      "multi_median=15\n",
      "\n",
      "for time in by_time:\n",
      "#    if(time==\"1346572800\"): break # single row for test \n",
      "    \n",
      "    print \n",
      "    print time, \"-\"*20\n",
      "    \n",
      "    tf=by_time[time]\n",
      "    timeframe={}\n",
      "    \n",
      "    # get user graph\n",
      "    timeframe[\"user_nodes\"]=[{\n",
      "                              \"name\":u[0],\n",
      "                              \"count\":u[1], \n",
      "                              \"province\":user_provinces[u[0]], \n",
      "                              \"community\":user_communities[u[0]],\n",
      "                              \"btw_cent\":users_btw_cent[u[0]]} \n",
      "                             for u in Counter(tf[\"user_nodes\"]).most_common() \n",
      "                             if u[0] in allowed_users]\n",
      "    \n",
      "    print \"%d users\"%len(timeframe[\"user_nodes\"])\n",
      "    \n",
      "    timeframe[\"user_edges\"]=[{\n",
      "                              \"source\":u[0][0],\n",
      "                              \"target\":u[0][1],\n",
      "                              \"weight\":u[1]\n",
      "                              } \n",
      "                              for u in Counter(tf[\"user_edges\"]).most_common()\n",
      "                              if u[0][0] in allowed_users and u[0][1] in allowed_users]\n",
      "    \n",
      "    print \"%d users edges\"%len(timeframe[\"user_edges\"])\n",
      "    \n",
      "    timeframe[\"provinces_edges\"]=[]\n",
      "    for u in timeframe[\"user_edges\"]:\n",
      "        try : source=user_provinces[u[\"source\"]]\n",
      "        except KeyError: pass\n",
      "        try : target=user_provinces[u[\"target\"]]\n",
      "        except KeyError: pass    \n",
      "        if source and target : timeframe[\"provinces_edges\"].append({\"source\":source,\"target\":target, \"weight\":u[\"weight\"]})\n",
      "    \n",
      "    print \"%d provinces edges\"%len(timeframe[\"provinces_edges\"])\n",
      "    \n",
      "    timeframe[\"words_nodes\"]=[{\n",
      "                                \"name\":w[0],\n",
      "                                \"count\":w[1],\n",
      "                                \"btw_cent\":words_btw_cent[str(wordIndex[w[0]])],\n",
      "                                \"community\":words_communities[str(wordIndex[w[0]])]\n",
      "                              } \n",
      "                              for w in Counter(tf[\"words_nodes\"]).most_common()\n",
      "                              if w[0] in words_allowed\n",
      "                              and w[1]>word_median ]\n",
      "    \n",
      "    print \"%d words\"%len(timeframe[\"words_nodes\"])\n",
      "    \n",
      "    words_edges_allowed= [w\n",
      "                          for w in tf[\"words_edges\"] \n",
      "                          if w[0]!=w[1]  \n",
      "                          and w[0] in words_allowed\n",
      "                          and w[1] in words_allowed\n",
      "                          ]\n",
      "    \n",
      "    words_edges_undirected=[]\n",
      "    for we in words_edges_allowed:\n",
      "        a=[we[0],we[1]]\n",
      "        a.sort()\n",
      "        words_edges_undirected.append(tuple(a))\n",
      "    \n",
      "    # for w in Counter(words_edges_undirected).most_common(): print w[0][0],w[0][1],w[1]\n",
      "        \n",
      "    timeframe[\"words_edges\"]=[{ \"source\":w[0][0],\n",
      "                               \"target\":w[0][1],\n",
      "                               \"weight\":w[1]}\n",
      "                                for w in Counter(words_edges_undirected).most_common()\n",
      "                                if w[1]>word_edge_median ]\n",
      "    \n",
      "    print \"%d words_edges\"%len(timeframe[\"words_edges\"])\n",
      "    \n",
      "    timeframe[\"multi_graph\"]=[{\"word\":w[0][0],\n",
      "                   \"user\":w[0][1],\n",
      "                   \"weight\":w[1]} \n",
      "                    for w in Counter(tf[\"words_to_users\"]).most_common()\n",
      "                    if w[0][1] in allowed_users \n",
      "                    and w[0][0] in words_allowed\n",
      "                    and w[1]>multi_median\n",
      "                    ]\n",
      "    print \"%d multilayer interactions\"%len(timeframe[\"multi_graph\"])\n",
      "    \n",
      "    # add province info\n",
      "    for edge in timeframe[\"multi_graph\"]:\n",
      "        # print edge[\"user\"]\n",
      "        try : province=user_provinces[edge[\"user\"]]\n",
      "        except KeyError: province=\"\"\n",
      "        edge[\"province\"]=province\n",
      "\n",
      "    timeframes.append({\"time\":time, \"data\":timeframe, \"count\":tf[\"count\"]})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1346367600 --------------------\n",
        "5 users\n",
        "4 users edges\n",
        "4 provinces edges\n",
        "13 words\n",
        "47 words_edges"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17 multilayer interactions\n",
        "\n",
        "1346572800 --------------------\n",
        "6 users\n",
        "2 users edges\n",
        "2 provinces edges\n",
        "10 words\n",
        "9 words_edges"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6 multilayer interactions\n",
        "\n",
        "1346180400 --------------------\n",
        "3 users\n",
        "1 users edges\n",
        "1 provinces edges\n",
        "23 words\n",
        "106 words_edges"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7 multilayer interactions\n",
        "\n",
        "1346094000 --------------------\n",
        "7 users\n",
        "3 users edges\n",
        "3 provinces edges\n",
        "38 words\n",
        "506 words_edges"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "41 multilayer interactions\n",
        "\n",
        "1345762800 --------------------\n",
        "1 users\n",
        "1 users edges\n",
        "0 provinces edges\n",
        "1 words\n",
        "0 words_edges"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0 multilayer interactions\n",
        "\n",
        "1345892400 --------------------\n",
        "0 users\n",
        "0 users edges\n",
        "0 provinces edges\n",
        "0 words\n",
        "0 words_edges\n",
        "0 multilayer interactions\n",
        "\n",
        "1345770000 --------------------\n",
        "0 users\n",
        "0 users edges\n",
        "0 provinces edges\n",
        "4 words\n",
        "14 words_edges\n",
        "0 multilayer interactions\n",
        "\n",
        "1345809600 --------------------\n",
        "0 users\n",
        "0 users edges\n",
        "0 provinces edges\n",
        "1 words\n",
        "0 words_edges"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0 multilayer interactions\n",
        "\n",
        "1346511600 --------------------\n",
        "3 users\n",
        "5 users edges\n",
        "5 provinces edges\n",
        "42 words\n",
        "337 words_edges"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "57 multilayer interactions\n",
        "\n",
        "1346054400 --------------------\n",
        "9 users\n",
        "9 users edges\n",
        "9 provinces edges\n",
        "17 words\n",
        "53 words_edges"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-31-2349570dcf47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_users\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_allowed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                     \u001b[1;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mmulti_median\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m                     ]\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"%d multilayer interactions\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"multi_graph\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timeframes_file=meme_path+\"/\"+meme_name+\"_timeframes.json\"\n",
      "\n",
      "with open(timeframes_file, 'w') as outfile:\n",
      "    json.dump(timeframes, outfile)\n",
      "    print \"json data have been saved to %s\"%(timeframes_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "json data have been saved to /home/clemsos/Dev/mitras/results/biaoge/biaoge_timeframes.json\n"
       ]
      }
     ],
     "prompt_number": 212
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lib.mongo import MongoDB\n",
      "\n",
      "# Variables\n",
      "collection=\"memes\"\n",
      "\n",
      "# Connect to Mongo\n",
      "db=MongoDB(\"weibodata\").db\n",
      "# count = db[collection].count()\n",
      "\n",
      "meme={\"name\":meme_name,\"data\":timeframes}\n",
      "db[collection].insert(meme)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Connecting to MongoDB... \n",
        "Connected successfully MongoDB at localhost:27017\n",
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 215,
       "text": [
        "ObjectId('535b9243ab4fc824862b58a3')"
       ]
      }
     ],
     "prompt_number": 215
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Requesting all tweets for protomeme : %s\"% meme_name\n",
      "\n",
      "query ={ \"name\" : meme_name }\n",
      "print \"%d results found\"%db[collection].find(query).count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Requesting all tweets for protomeme : biaoge\n",
        "1 results found\n"
       ]
      }
     ],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}