{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Time-based data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "import os, csv, json\n",
      "from time import time \n",
      "import datetime\n",
      "from collections import Counter\n",
      "import lib.tweetminer as minetweet\n",
      "from lib.users import UserAPI\n",
      "import networkx as nx\n",
      "import community\n",
      "from lib.nlp import NLPMiner\n",
      "import locale\n",
      "\n",
      "results_path=\"/home/clemsos/Dev/mitras/results/\"\n",
      "meme_names=[\"biaoge\"]\n",
      "# meme_names=[ meme for meme in os.listdir(results_path) if meme[-3:] != \"csv\"]\n",
      "# meme_names=[\n",
      "#  'biaoge',\n",
      "#  'thevoice',\n",
      "#  'moyan',\n",
      "#  'hougong',\n",
      "#  'gangnam',\n",
      "#  'sextape',\n",
      "#  'dufu',\n",
      "#  'ccp',\n",
      "#  'yuanfang',\n",
      "#  'qiegao']\n",
      "\n",
      "\n",
      "print meme_names\n",
      "\n",
      "\n",
      "t0=time()\n",
      "minetweet.init_tweet_regex()\n",
      "\n",
      "\n",
      "locale.setlocale(locale.LC_ALL, \"\")\n",
      "\n",
      "nlp=NLPMiner()\n",
      "\n",
      "stoplist=[i.strip() for i in open(\"lib/stopwords/zh-stopwords\",\"r\")]\n",
      "stoplist+=[i.strip() for i in open(\"lib/stopwords/stopwords.txt\",\"r\")]\n",
      "stoplist+=[\"\u8f6c\u53d1\",\"\u5fae\u535a\",\"\u8bf4 \",\"\u4e00\u4e2a\",\"\u3010 \",\"\u5e74 \",\"\u8f6c \",\"\u8bf7\",\"\uff02 \",\"\u95ee\u9898\",\"\u77e5\u9053\",\"\u4e2d \",\"\u5df2\u7ecf\",\"\u73b0\u5728\",\"\u8bf4\",\"\u3010\",'\uff02',\"\u5e74\",\"\u4e2d\",\"\u4eca\u5929\",\"\u5e94\u8be5\",\"\u771f\u7684\",\"\u6708\",\"\u5e0c\u671b\",\"\u60f3\",\"\u65e5\",\"\u8fd9\u662f\",\"\u592a\",\"\u8f6c\",\"\u652f\u6301\"]\n",
      "# stoplist+=[\"\u4e8b\u513f\",\"\u4e2d\u56fd\"]\n",
      "\n",
      "\n",
      "api=UserAPI()\n",
      "words_users_time=[]\n",
      "\n",
      "def get_province(_userid):\n",
      "    province_code= api.get_province(_userid)\n",
      "    # print province_code\n",
      "    try :\n",
      "        return api.provinces[province_code]\n",
      "    except KeyError :\n",
      "        return 0\n",
      "        pass\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "init tweet entities regex\n",
        "['biaoge']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "init tweet entities regex\n",
        "init NLP toolkit\n",
        "User API instance\n",
        "\n",
        "Connecting to MongoDB... \n",
        "Connected successfully MongoDB at localhost:27017\n",
        "\n",
        "Total users in the db : 14388484\n",
        "------------\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "meme_name=\"biaoge\"\n",
      "\n",
      "# Init\n",
      "# tstart=time()\n",
      "print \"Processing meme '%s'\"%meme_name\n",
      "\n",
      "# files names\n",
      "meme_path=outfile=results_path+meme_name\n",
      "meme_csv=meme_path+\"/\"+meme_name+\".csv\"\n",
      "\n",
      "jsondata={}\n",
      "jsondata[\"meme_name\"]=meme_name\n",
      "\n",
      "users=[]\n",
      "users_edges=[]\n",
      "\n",
      "words=[]\n",
      "words_edges=[]\n",
      "words_users=[]\n",
      "\n",
      "count=0\n",
      "\n",
      "#\u00a0 words_time={}\n",
      "# user_edges_time=[]\n",
      "# words_users_time={}\n",
      "\n",
      "by_time={}\n",
      "\n",
      "# process the data\n",
      "with open(meme_csv, 'rb') as csvfile:\n",
      "    memecsv=csv.reader(csvfile)\n",
      "    memecsv.next() # skip headers\n",
      "\n",
      "    for row in memecsv:\n",
      "        # extract text\n",
      "        t=row[1]    \n",
      "        count+=1\n",
      "        \n",
      "        # time (round and store)\n",
      "        d=datetime.datetime.strptime(row[9], \"%Y-%m-%dT%H:%M:%S\")\n",
      "        day = datetime.datetime(d.year,d.month,d.day,d.hour,0,0)\n",
      "        timestamp=day.strftime(\"%s\")\n",
      "        \n",
      "        # regexp extract tweet entities\n",
      "        mentions,urls,hashtags,clean=minetweet.extract_tweet_entities(t)\n",
      "        \n",
      "        # extract text \n",
      "        dico=nlp.extract_dictionary(clean)\n",
      "\n",
      "        # remove stopwords and get clean dico\n",
      "        clean_dico=nlp.remove_stopwords(dico)\n",
      "        \n",
      "        # remove more stopwords\n",
      "        tmp_words=[w for w in clean_dico if w.encode('utf-8') not in stoplist and w[0] != \"u\" ]\n",
      "        words+=tmp_words # global list for counter            \n",
      "\n",
      "        # User diffusion graph\n",
      "        user_diff=[]\n",
      "        for mention in mentions:\n",
      "            user_edges.append((row[0],mention))\n",
      "            # user_edges_time.append((row[0],mention,timestamp))\n",
      "            if mention not in user_diff : user_diff.append(mention)\n",
      "\n",
      "            # retweeted_uid\n",
      "        if row[7] != \"\" : \n",
      "            user_edges.append((row[7],row[0]))\n",
      "            # user_edges_time.append((row[7],row[0],timestamp))\n",
      "            if row[7] not in user_diff : user_diff.append(row[7])\n",
      "        \n",
      "        users+=user_diff # store all users\n",
      "        \n",
      "        words_to_words=[]\n",
      "        words_to_users=[]\n",
      "\n",
      "        for w in tmp_words :\n",
      "            \n",
      "            # word edges\n",
      "            words_to_words+=[(w,t) for t in tmp_words if t!=w]\n",
      "            \n",
      "            # word to users\n",
      "            words_to_users+=[(w,u) for u in user_diff]\n",
      "        \n",
      "        words_edges+=words_to_words\n",
      "        words_users+=words_to_users\n",
      "        \n",
      "        # store data by time\n",
      "        try : by_time[timestamp]\n",
      "        except KeyError: by_time[timestamp]={}\n",
      "            \n",
      "        # users edges\n",
      "        try: by_time[timestamp][\"user_edges\"]\n",
      "        except KeyError: by_time[timestamp][\"user_edges\"]=[]\n",
      "        by_time[timestamp][\"user_edges\"]+=user_edges\n",
      "        \n",
      "        # users nodes\n",
      "        try: by_time[timestamp][\"user_nodes\"]\n",
      "        except KeyError: by_time[timestamp][\"user_nodes\"]=[]\n",
      "        by_time[timestamp][\"user_nodes\"]+=user_diff\n",
      "        \n",
      "        # words nodes\n",
      "        try: by_time[timestamp][\"words_nodes\"]\n",
      "        except KeyError: by_time[timestamp][\"words_nodes\"]=[]\n",
      "        by_time[timestamp][\"words_nodes\"]+=tmp_words\n",
      "        \n",
      "        # word edges\n",
      "        try: by_time[timestamp][\"words_edges\"]\n",
      "        except KeyError: by_time[timestamp][\"words_edges\"]=[]\n",
      "        by_time[timestamp][\"words_edges\"]+=words_to_words\n",
      "        \n",
      "        # word edges\n",
      "        try: by_time[timestamp][\"words_to_users\"]\n",
      "        except KeyError: by_time[timestamp][\"words_to_users\"]=[]\n",
      "        by_time[timestamp][\"words_to_users\"]+=words_to_users\n",
      "\n",
      "print \"processing done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing meme 'biaoge'\n",
        "processing done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parse provinces for all users\n",
      "\n",
      "user_provinces={}\n",
      "unique_users=[u[0] for u in Counter(users).most_common()]\n",
      "for user in unique_users:\n",
      "    province=get_province(user)\n",
      "    user_provinces[user]=province"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# limit data size\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parse data correctly\n",
      "\n",
      "# for time in by_time: print time\n",
      "'''\n",
      "1345446000\n",
      "1345492800\n",
      "1346594400\n",
      "1346605200\n",
      "1345478400\n",
      "1346353200\n",
      "1345734000\n",
      "1346482800\n",
      "1345651200\n",
      "1346047200\n",
      "'''\n",
      "# for a single time\n",
      "time=\"1346594400\"\n",
      "tf=by_time[time]\n",
      "timeframe={}\n",
      "\n",
      "# get user graph\n",
      "timeframe[\"user_nodes\"]=[ {\"name\":u[0],\"count\":u[1], \"province\":user_provinces[u[0]]} for u in Counter(tf[\"user_nodes\"]).most_common()]\n",
      "\n",
      "timeframe[\"user_edges\"]=[ { \"source\":u[0][0],\n",
      "               \"target\":u[0][0],\n",
      "               \"weight\":u[1]} \n",
      "                for u in Counter(tf[\"user_edges\"]).most_common()]\n",
      "\n",
      "timeframe[\"provinces_edges\"]=[]\n",
      "for u in timeframe[\"user_edges\"]:\n",
      "    try : source=user_provinces[u[\"source\"]]\n",
      "    except KeyError: pass\n",
      "    try : target=user_provinces[u[\"target\"]]\n",
      "    except KeyError: pass    \n",
      "    if source and target : timeframe[\"provinces_edges\"].append({\"source\":source,\"target\":target, \"weight\":u[\"weight\"]})\n",
      "\n",
      "timeframe[\"words_nodes\"]=[ {\"name\":w[0],\"count\":w[1]} for w in Counter(tf[\"words_nodes\"]).most_common()]\n",
      "\n",
      "timeframe[\"words_edges\"]=[{ \"source\":w[0][0],\n",
      "               \"target\":w[0][1],\n",
      "               \"weight\":w[1]} \n",
      "                for w in Counter(tf[\"words_edges\"]).most_common() if w[0][0]!=w[0][1] ]\n",
      "\n",
      "timeframe[\"multi_graph\"]=[{\"word\":w[0][0],\n",
      "               \"user\":w[0][1],\n",
      "               \"weight\":w[1]} \n",
      "                for w in Counter(tf[\"words_to_users\"]).most_common()]\n",
      "\n",
      "# add province info\n",
      "for edge in timeframe[\"multi_graph\"]:\n",
      "    # print edge[\"user\"]\n",
      "    try : province=user_provinces[edge[\"user\"]]\n",
      "    except KeyError: province=\"\"\n",
      "    edge[\"province\"]=province\n",
      "\n",
      "for key in timeframe: print time,key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "user_edges\n",
        "words_edges\n",
        "multi_graph\n",
        "words_nodes\n",
        "user_nodes\n",
        "provinces_edges\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# collect province for all users\n",
      "if generate_users_map :\n",
      "    province=get_province(row[0])\n",
      "    if province!=0 : user_provinces.append(province)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}